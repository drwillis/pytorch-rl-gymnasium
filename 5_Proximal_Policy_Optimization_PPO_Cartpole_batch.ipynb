{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c096b86-009f-42bf-940f-90cbb9b85356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "PPO implementation for CartPole-v1 (gymnasium).\n",
    "- Batched rollouts, clipped surrogate, multiple epochs & minibatches.\n",
    "- Shared actor-critic network.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6311d-81fa-41ab-a694-02773df200d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Device & seed\n",
    "# ----------------------------\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # MPS may not be desirable for RL but we check\n",
    "    try:\n",
    "        if torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc3ca0e-780e-429d-b18f-a35c22de0f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7cf9e-c211-4ede-9cb6-a8123b95368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Actor-Critic network (shared body)\n",
    "# ----------------------------\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # actor head\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        # critic head\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # initialize\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, obs]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.actor(x)         # action logits\n",
    "        value = self.critic(x)         # state value (scalar)\n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893fe83c-1b53-4033-a484-1fe4a1667e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Returns & helper functions\n",
    "# ----------------------------\n",
    "def calculate_returns(rewards, gamma, device, normalize=True):\n",
    "    \"\"\"Compute discounted returns (per episode).\"\"\"\n",
    "    returns = []\n",
    "    R = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "    if normalize:\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80cadf4-cc2a-40b3-9b9b-952e7664edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episodes(env, policy, n_episodes, device, gamma):\n",
    "    \"\"\"\n",
    "    Collect n_episodes complete episodes (sequentially) and return concatenated\n",
    "    tensors for states, actions, old_log_probs, returns, values, and episode rewards.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    log_probs_old = []\n",
    "    returns_list = []\n",
    "    values = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        ep_states = []\n",
    "        ep_actions = []\n",
    "        ep_log_probs = []\n",
    "        ep_rewards = []\n",
    "        ep_values = []\n",
    "\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        while not done:\n",
    "            s_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)  # [1, obs]\n",
    "            logits, value = policy(s_t)  # logits: [1, A], value: [1,1]\n",
    "            dist = distributions.Categorical(logits=logits)\n",
    "            action = dist.sample()       # [1]\n",
    "            logp = dist.log_prob(action) # [1]\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "\n",
    "            ep_states.append(s_t)                # list of tensors [1,obs]\n",
    "            ep_actions.append(action.squeeze(0)) # tensor scalar\n",
    "            ep_log_probs.append(logp.squeeze(0)) # tensor scalar\n",
    "            ep_rewards.append(reward)\n",
    "            ep_values.append(value.squeeze(0))   # scalar tensor\n",
    "            \n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # episode end: compute returns for episode\n",
    "        ep_returns = calculate_returns(ep_rewards, gamma, device, normalize=False)  # normalized later optionally\n",
    "        episode_rewards.append(sum(ep_rewards))\n",
    "\n",
    "        # stack episode tensors and append to global lists\n",
    "        states.append(torch.cat(ep_states, dim=0))         # [T, obs]\n",
    "        actions.append(torch.stack(ep_actions))            # [T]\n",
    "        log_probs_old.append(torch.stack(ep_log_probs))    # [T]\n",
    "        returns_list.append(ep_returns)                    # [T]\n",
    "        values.append(torch.cat(ep_values).detach())     # [T] detach values when used as baseline\n",
    "\n",
    "    # concatenate across episodes -> final batch\n",
    "    states_b = torch.cat(states, dim=0)           # [N, obs]\n",
    "    actions_b = torch.cat(actions, dim=0)         # [N]\n",
    "    log_probs_old_b = torch.cat(log_probs_old, dim=0).detach()   # [N]\n",
    "    returns_b = torch.cat(returns_list, dim=0)    # [N]\n",
    "    values_b = torch.cat(values, dim=0)           # [N]\n",
    "    return states_b, actions_b, log_probs_old_b, returns_b, values_b, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d38e71-a13a-4b08-9d18-18e66e271d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# PPO update\n",
    "# ----------------------------\n",
    "def ppo_update(policy, optimizer, states, actions, old_log_probs, returns, values,\n",
    "               ppo_epochs=4, minibatch_size=64, clip_eps=0.2,\n",
    "               vf_coef=0.5, ent_coef=0.01, max_grad_norm=0.5):\n",
    "\n",
    "    device = states.device\n",
    "    advantages = (returns - values).squeeze()\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    N = states.shape[0]\n",
    "    total_policy_loss = total_value_loss = total_entropy = 0.0\n",
    "    n_updates = 0\n",
    "\n",
    "    # print(states.shape, actions.shape, old_log_probs.shape, returns.shape, values.shape, advantages.shape)\n",
    "\n",
    "    for epoch in range(ppo_epochs):\n",
    "        perm = torch.randperm(N, device=device)\n",
    "        for start in range(0, N, minibatch_size):\n",
    "            idx = perm[start:start + minibatch_size]\n",
    "\n",
    "            s_mb = states[idx]\n",
    "            a_mb = actions[idx].squeeze()\n",
    "            old_logp_mb = old_log_probs[idx].squeeze()\n",
    "            return_mb = returns[idx].squeeze()\n",
    "            adv_mb = advantages[idx].squeeze()\n",
    "            logits, value_pred = policy(s_mb)\n",
    "            value_pred = value_pred.squeeze(-1)\n",
    "\n",
    "            dist = distributions.Categorical(logits=logits)\n",
    "            new_logp_mb = dist.log_prob(a_mb)\n",
    "            entropy_mb = dist.entropy().mean()\n",
    "\n",
    "            ratio = (new_logp_mb - old_logp_mb).exp()\n",
    "            surr1 = ratio * adv_mb\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * adv_mb\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            value_loss = F.mse_loss(value_pred, return_mb, reduction='mean')\n",
    "\n",
    "            loss = policy_loss + vf_coef * value_loss - ent_coef * entropy_mb\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_policy_loss += policy_loss.item()\n",
    "            total_value_loss += value_loss.item()\n",
    "            total_entropy += entropy_mb.item()\n",
    "            n_updates += 1\n",
    "\n",
    "    return total_policy_loss / n_updates, total_value_loss / n_updates, total_entropy / n_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dace702-3bfa-4eb2-b4b8-bcc51aaeb7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Main training loop\n",
    "# ----------------------------\n",
    "\n",
    "# envs\n",
    "train_env = gymnasium.make('CartPole-v1')\n",
    "test_env = gymnasium.make('CartPole-v1')\n",
    "\n",
    "# hyperparams\n",
    "SEED = 1234\n",
    "set_seed(SEED)\n",
    "train_env.reset(seed=SEED)\n",
    "test_env.reset(seed=SEED + 1)\n",
    "\n",
    "device = get_device()\n",
    "print(\"Device:\", device)\n",
    "\n",
    "obs_dim = train_env.observation_space.shape[0]\n",
    "action_dim = train_env.action_space.n\n",
    "\n",
    "policy = ActorCritic(obs_dim, hidden_dim=128, action_dim=action_dim).to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=3e-4)\n",
    "\n",
    "max_updates = 500           # number of update iterations (not episodes)\n",
    "batch_episodes = 8           # number of episodes per update (increase to reduce variance)\n",
    "ppo_epochs = 4\n",
    "minibatch_size = 64\n",
    "gamma = 0.99\n",
    "clip_eps = 0.2\n",
    "vf_coef = 0.5\n",
    "ent_coef = 0.01\n",
    "\n",
    "print_every = 1\n",
    "reward_threshold = 475\n",
    "recent_rewards = []\n",
    "\n",
    "train_rewards_log = []\n",
    "test_rewards_log = []\n",
    "\n",
    "episodes_seen = 0\n",
    "for update in range(1, max_updates + 1):\n",
    "    # collect rollouts (batch of full episodes)\n",
    "    states_b, actions_b, old_logp_b, returns_b, values_b, episode_rewards = collect_episodes(\n",
    "        train_env, policy, n_episodes=batch_episodes, device=device, gamma=gamma\n",
    "    )\n",
    "    episodes_seen += len(episode_rewards)\n",
    "    train_rewards_log.extend(episode_rewards)\n",
    "    recent_rewards.extend(episode_rewards)\n",
    "    # normalize returns across the whole batch (optional but helps)\n",
    "    returns_b = (returns_b - returns_b.mean()) / (returns_b.std() + 1e-8)\n",
    "\n",
    "    # PPO update\n",
    "    p_loss, v_loss, ent = ppo_update(\n",
    "        policy, optimizer,\n",
    "        states_b, actions_b, old_logp_b, returns_b, values_b,\n",
    "        ppo_epochs=ppo_epochs, minibatch_size=minibatch_size, clip_eps=clip_eps,\n",
    "        vf_coef=vf_coef, ent_coef=ent_coef\n",
    "    )\n",
    "\n",
    "    # evaluate on test env periodically\n",
    "    if update % print_every == 0:\n",
    "        # run one deterministic evaluation episode\n",
    "        policy.eval()\n",
    "        s, _ = test_env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            s_t = torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                logits, _ = policy(s_t)\n",
    "                action = torch.argmax(logits, dim=-1).item()\n",
    "            s, r, term, trunc, _ = test_env.step(action)\n",
    "            done = term or trunc\n",
    "            total_reward += r\n",
    "        test_rewards_log.append(total_reward)\n",
    "\n",
    "        recent_mean = np.mean(recent_rewards[-100:]) if len(recent_rewards) > 0 else 0.0\n",
    "        print(f\"Update {update:4d} | Episodes {episodes_seen:4d} | AvgEpReward(last {min(100,len(recent_rewards))}): {recent_mean:6.2f} | \"\n",
    "              f\"P_loss {p_loss:.4f} V_loss {v_loss:.4f} Ent {ent:.4f} | TestReward {total_reward:.1f}\")\n",
    "\n",
    "    # stopping criteria using last 100 episode average\n",
    "    if len(recent_rewards) >= 100 and np.mean(recent_rewards[-100:]) >= reward_threshold:\n",
    "        print(f\"Stopping at update {update} — avg 100-ep reward {np.mean(recent_rewards[-100:]):.1f}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b189dc3-4ffc-455f-afea-0b7f74197d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & test rewards\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_rewards_log, label=\"Train episode reward\")\n",
    "plt.plot(np.arange(0, len(test_rewards_log)) * print_every * batch_episodes, test_rewards_log, label=\"Test reward (periodic)\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
