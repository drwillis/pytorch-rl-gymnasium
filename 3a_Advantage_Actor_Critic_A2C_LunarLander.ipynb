{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/\n",
    "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f\n",
    "http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf.pdf\n",
    "https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/\n",
    "https://danieltakeshi.github.io/2018/06/28/a2c-a3c/\n",
    "https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/\n",
    "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/a2c_ppo_acktr/algo/a2c_acktr.py\n",
    "https://github.com/higgsfield/RL-Adventure-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import ale_py\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Device setup\n",
    "# ----------------------------\n",
    "def get_device():\n",
    "    \"\"\"Select available device (CUDA, MPS for Apple, or CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():  # macOS Metal\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.version.hip is not None and torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")  # AMD ROCm usually appears as CUDA device\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Seeding for reproducibility\n",
    "# ----------------------------\n",
    "def set_seed(seed):\n",
    "    \"\"\"Sets seed for reproducibility across libraries and devices.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, actor, critic):\n",
    "        super().__init__()\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        \n",
    "    def forward(self, state):\n",
    "        action_pred = self.actor(state)\n",
    "        value_pred = self.critic(state)\n",
    "        \n",
    "        return action_pred, value_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_episode(env, policy, optimizer, gamma, device):\n",
    "    \"\"\"Train policy for one episode.\"\"\"            \n",
    "    policy.train()\n",
    "    log_prob_actions, rewards, values = [], [], []\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    while not done:\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        action_pred, value_pred = policy(state)                \n",
    "        action_prob = F.softmax(action_pred, dim = -1)\n",
    "        dist = distributions.Categorical(action_prob)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        next_state, reward, done, truncated, _ = env.step(action.item())\n",
    "        done = done or truncated\n",
    "\n",
    "        log_prob_actions.append(dist.log_prob(action))\n",
    "        rewards.append(reward)\n",
    "        values.append(value_pred.squeeze(0))\n",
    "        state = next_state\n",
    "    \n",
    "    log_prob_actions = torch.stack(log_prob_actions)\n",
    "    values = torch.cat(values)\n",
    "    \n",
    "    returns = calculate_returns(rewards, gamma, device, True)\n",
    "    advantages = calculate_advantages(returns, values, True)    \n",
    "    policy_loss, value_loss = update_policy(advantages, log_prob_actions, returns, values, optimizer)\n",
    "\n",
    "    return policy_loss, value_loss, sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Training and evaluation\n",
    "# ----------------------------\n",
    "def calculate_returns(rewards, gamma, device, normalize=True):\n",
    "    \"\"\"Compute discounted returns for an episode.\"\"\"\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "    if normalize:\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantages(returns, values, normalize = True):\n",
    "    advantages = returns - values.detach()\n",
    "    if normalize:\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(advantages, log_prob_actions, returns, values, optimizer):\n",
    "    \"\"\"Compute loss and update policy and value parameters.\"\"\"    \n",
    "    policy_loss = -(advantages * log_prob_actions).mean()    \n",
    "    value_loss = F.smooth_l1_loss(values, returns, reduction='mean')\n",
    "    loss = policy_loss + 0.5 * value_loss;\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # policy_loss.backward()\n",
    "    # value_loss.backward()\n",
    "    optimizer.step()\n",
    "    return policy_loss.item(), value_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy, device):\n",
    "    \"\"\"Evaluate policy (greedy).\"\"\"        \n",
    "    policy.eval()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action_logits, _ = policy(state_tensor)\n",
    "            action = torch.argmax(F.softmax(action_logits, dim=-1)).item()\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        done = done or truncated\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "| Episode:  10 | Mean Train Rewards:  -290.2 | Mean Test Rewards:  -488.0 |\n",
      "| Episode:  20 | Mean Train Rewards:  -237.7 | Mean Test Rewards:  -478.3 |\n",
      "| Episode:  30 | Mean Train Rewards:  -228.0 | Mean Test Rewards:  -493.2 |\n",
      "| Episode:  40 | Mean Train Rewards:  -220.9 | Mean Test Rewards:  -464.5 |\n",
      "| Episode:  50 | Mean Train Rewards:  -212.0 | Mean Test Rewards:  -462.5 |\n",
      "| Episode:  60 | Mean Train Rewards:  -213.7 | Mean Test Rewards:  -607.3 |\n",
      "| Episode:  70 | Mean Train Rewards:  -204.3 | Mean Test Rewards:  -776.8 |\n",
      "| Episode:  80 | Mean Train Rewards:  -221.4 | Mean Test Rewards:  -903.2 |\n",
      "| Episode:  90 | Mean Train Rewards:  -206.7 | Mean Test Rewards:  -822.8 |\n",
      "| Episode: 100 | Mean Train Rewards:  -185.4 | Mean Test Rewards:  -748.2 |\n",
      "| Episode: 110 | Mean Train Rewards:  -186.0 | Mean Test Rewards:  -685.8 |\n",
      "| Episode: 120 | Mean Train Rewards:  -209.8 | Mean Test Rewards:  -736.4 |\n",
      "| Episode: 130 | Mean Train Rewards:  -197.3 | Mean Test Rewards:  -976.1 |\n",
      "| Episode: 140 | Mean Train Rewards:  -187.0 | Mean Test Rewards:  -958.6 |\n",
      "| Episode: 150 | Mean Train Rewards:  -234.6 | Mean Test Rewards:  -793.3 |\n",
      "| Episode: 160 | Mean Train Rewards:  -255.4 | Mean Test Rewards:  -756.9 |\n",
      "| Episode: 170 | Mean Train Rewards:  -241.7 | Mean Test Rewards:  -846.4 |\n",
      "| Episode: 180 | Mean Train Rewards:  -246.0 | Mean Test Rewards:  -946.4 |\n",
      "| Episode: 190 | Mean Train Rewards:  -224.0 | Mean Test Rewards:  -889.5 |\n",
      "| Episode: 200 | Mean Train Rewards:  -237.0 | Mean Test Rewards:  -829.0 |\n",
      "| Episode: 210 | Mean Train Rewards:  -229.3 | Mean Test Rewards:  -956.4 |\n",
      "| Episode: 220 | Mean Train Rewards:  -242.0 | Mean Test Rewards:  -885.0 |\n",
      "| Episode: 230 | Mean Train Rewards:  -273.3 | Mean Test Rewards:  -909.6 |\n",
      "| Episode: 240 | Mean Train Rewards:  -251.0 | Mean Test Rewards:  -878.8 |\n",
      "| Episode: 250 | Mean Train Rewards:  -247.8 | Mean Test Rewards:  -852.2 |\n",
      "| Episode: 260 | Mean Train Rewards:  -209.4 | Mean Test Rewards:  -775.1 |\n",
      "| Episode: 270 | Mean Train Rewards:  -212.0 | Mean Test Rewards:  -726.4 |\n",
      "| Episode: 280 | Mean Train Rewards:  -254.0 | Mean Test Rewards:  -714.1 |\n",
      "| Episode: 290 | Mean Train Rewards:  -295.6 | Mean Test Rewards:  -700.8 |\n",
      "| Episode: 300 | Mean Train Rewards:  -325.1 | Mean Test Rewards:  -779.5 |\n",
      "| Episode: 310 | Mean Train Rewards:  -336.0 | Mean Test Rewards:  -802.9 |\n",
      "| Episode: 320 | Mean Train Rewards:  -271.8 | Mean Test Rewards:  -896.1 |\n",
      "| Episode: 330 | Mean Train Rewards:  -232.9 | Mean Test Rewards:  -994.4 |\n",
      "| Episode: 340 | Mean Train Rewards:  -182.8 | Mean Test Rewards: -1065.1 |\n",
      "| Episode: 350 | Mean Train Rewards:  -236.5 | Mean Test Rewards: -1248.2 |\n",
      "| Episode: 360 | Mean Train Rewards:  -229.1 | Mean Test Rewards: -1122.5 |\n",
      "| Episode: 370 | Mean Train Rewards:  -237.9 | Mean Test Rewards:  -779.4 |\n",
      "| Episode: 380 | Mean Train Rewards:  -215.7 | Mean Test Rewards:  -802.2 |\n",
      "| Episode: 390 | Mean Train Rewards:  -219.3 | Mean Test Rewards:  -902.5 |\n",
      "| Episode: 400 | Mean Train Rewards:  -294.2 | Mean Test Rewards: -1143.2 |\n",
      "| Episode: 410 | Mean Train Rewards:  -310.9 | Mean Test Rewards: -1151.5 |\n",
      "| Episode: 420 | Mean Train Rewards:  -275.0 | Mean Test Rewards:  -929.4 |\n",
      "| Episode: 430 | Mean Train Rewards:  -263.0 | Mean Test Rewards:  -773.8 |\n",
      "| Episode: 440 | Mean Train Rewards:  -253.5 | Mean Test Rewards:  -736.0 |\n",
      "| Episode: 450 | Mean Train Rewards:  -232.4 | Mean Test Rewards:  -806.3 |\n",
      "| Episode: 460 | Mean Train Rewards:  -212.4 | Mean Test Rewards:  -814.8 |\n",
      "| Episode: 470 | Mean Train Rewards:  -241.5 | Mean Test Rewards:  -644.4 |\n",
      "| Episode: 480 | Mean Train Rewards:  -217.0 | Mean Test Rewards:  -564.2 |\n",
      "| Episode: 490 | Mean Train Rewards:  -250.2 | Mean Test Rewards:  -575.5 |\n",
      "| Episode: 500 | Mean Train Rewards:  -255.3 | Mean Test Rewards:  -604.2 |\n",
      "| Episode: 510 | Mean Train Rewards:  -255.6 | Mean Test Rewards:  -651.3 |\n",
      "| Episode: 520 | Mean Train Rewards:  -263.5 | Mean Test Rewards:  -616.6 |\n",
      "| Episode: 530 | Mean Train Rewards:  -232.7 | Mean Test Rewards:  -632.3 |\n",
      "| Episode: 540 | Mean Train Rewards:  -240.9 | Mean Test Rewards:  -564.1 |\n",
      "| Episode: 550 | Mean Train Rewards:  -254.9 | Mean Test Rewards:  -544.3 |\n",
      "| Episode: 560 | Mean Train Rewards:  -257.7 | Mean Test Rewards:  -541.1 |\n",
      "| Episode: 570 | Mean Train Rewards:  -257.2 | Mean Test Rewards:  -526.6 |\n",
      "| Episode: 580 | Mean Train Rewards:  -226.2 | Mean Test Rewards:  -530.4 |\n",
      "| Episode: 590 | Mean Train Rewards:  -270.7 | Mean Test Rewards:  -586.4 |\n",
      "| Episode: 600 | Mean Train Rewards:  -327.8 | Mean Test Rewards:  -591.0 |\n",
      "| Episode: 610 | Mean Train Rewards:  -289.0 | Mean Test Rewards:  -630.1 |\n",
      "| Episode: 620 | Mean Train Rewards:  -290.6 | Mean Test Rewards:  -613.0 |\n",
      "| Episode: 630 | Mean Train Rewards:  -267.9 | Mean Test Rewards:  -582.2 |\n",
      "| Episode: 640 | Mean Train Rewards:  -297.8 | Mean Test Rewards:  -572.6 |\n",
      "| Episode: 650 | Mean Train Rewards:  -270.9 | Mean Test Rewards:  -609.6 |\n",
      "| Episode: 660 | Mean Train Rewards:  -257.1 | Mean Test Rewards:  -588.1 |\n",
      "| Episode: 670 | Mean Train Rewards:  -263.0 | Mean Test Rewards:  -612.2 |\n",
      "| Episode: 680 | Mean Train Rewards:  -245.1 | Mean Test Rewards:  -578.3 |\n",
      "| Episode: 690 | Mean Train Rewards:  -234.0 | Mean Test Rewards:  -611.3 |\n",
      "| Episode: 700 | Mean Train Rewards:  -257.0 | Mean Test Rewards:  -622.3 |\n",
      "| Episode: 710 | Mean Train Rewards:  -241.2 | Mean Test Rewards:  -738.5 |\n",
      "| Episode: 720 | Mean Train Rewards:  -239.0 | Mean Test Rewards:  -835.9 |\n",
      "| Episode: 730 | Mean Train Rewards:  -214.3 | Mean Test Rewards:  -823.7 |\n",
      "| Episode: 740 | Mean Train Rewards:  -202.0 | Mean Test Rewards:  -701.5 |\n",
      "| Episode: 750 | Mean Train Rewards:  -189.6 | Mean Test Rewards:  -898.4 |\n",
      "| Episode: 760 | Mean Train Rewards:  -216.4 | Mean Test Rewards:  -886.7 |\n",
      "| Episode: 770 | Mean Train Rewards:  -248.1 | Mean Test Rewards:  -760.6 |\n",
      "| Episode: 780 | Mean Train Rewards:  -269.8 | Mean Test Rewards:  -761.0 |\n",
      "| Episode: 790 | Mean Train Rewards:  -263.8 | Mean Test Rewards:  -765.8 |\n",
      "| Episode: 800 | Mean Train Rewards:  -243.2 | Mean Test Rewards:  -730.5 |\n",
      "| Episode: 810 | Mean Train Rewards:  -232.7 | Mean Test Rewards:  -885.5 |\n",
      "| Episode: 820 | Mean Train Rewards:  -261.4 | Mean Test Rewards:  -862.8 |\n",
      "| Episode: 830 | Mean Train Rewards:  -255.2 | Mean Test Rewards:  -658.2 |\n",
      "| Episode: 840 | Mean Train Rewards:  -229.8 | Mean Test Rewards:  -735.2 |\n",
      "| Episode: 850 | Mean Train Rewards:  -219.5 | Mean Test Rewards:  -733.1 |\n",
      "| Episode: 860 | Mean Train Rewards:  -211.5 | Mean Test Rewards:  -868.9 |\n",
      "| Episode: 870 | Mean Train Rewards:  -232.6 | Mean Test Rewards:  -766.1 |\n",
      "| Episode: 880 | Mean Train Rewards:  -266.2 | Mean Test Rewards:  -748.9 |\n",
      "| Episode: 890 | Mean Train Rewards:  -235.5 | Mean Test Rewards:  -811.4 |\n",
      "| Episode: 900 | Mean Train Rewards:  -220.6 | Mean Test Rewards:  -841.9 |\n",
      "| Episode: 910 | Mean Train Rewards:  -268.9 | Mean Test Rewards:  -713.9 |\n",
      "| Episode: 920 | Mean Train Rewards:  -266.6 | Mean Test Rewards:  -787.1 |\n",
      "| Episode: 930 | Mean Train Rewards:  -262.0 | Mean Test Rewards:  -817.1 |\n",
      "| Episode: 940 | Mean Train Rewards:  -266.0 | Mean Test Rewards:  -804.4 |\n",
      "| Episode: 950 | Mean Train Rewards:  -284.2 | Mean Test Rewards: -1067.3 |\n",
      "| Episode: 960 | Mean Train Rewards:  -249.3 | Mean Test Rewards: -1199.0 |\n",
      "| Episode: 970 | Mean Train Rewards:  -264.5 | Mean Test Rewards: -1260.8 |\n",
      "| Episode: 980 | Mean Train Rewards:  -237.8 | Mean Test Rewards:  -899.5 |\n",
      "| Episode: 990 | Mean Train Rewards:  -232.8 | Mean Test Rewards:  -788.0 |\n",
      "| Episode: 1000 | Mean Train Rewards:  -245.2 | Mean Test Rewards:  -819.2 |\n",
      "| Episode: 1010 | Mean Train Rewards:  -261.4 | Mean Test Rewards:  -959.1 |\n",
      "| Episode: 1020 | Mean Train Rewards:  -316.1 | Mean Test Rewards:  -940.3 |\n",
      "| Episode: 1030 | Mean Train Rewards:  -303.4 | Mean Test Rewards:  -803.7 |\n",
      "| Episode: 1040 | Mean Train Rewards:  -286.6 | Mean Test Rewards:  -719.0 |\n",
      "| Episode: 1050 | Mean Train Rewards:  -277.5 | Mean Test Rewards:  -715.9 |\n",
      "| Episode: 1060 | Mean Train Rewards:  -295.1 | Mean Test Rewards:  -813.9 |\n",
      "| Episode: 1070 | Mean Train Rewards:  -327.5 | Mean Test Rewards:  -843.7 |\n",
      "| Episode: 1080 | Mean Train Rewards:  -303.4 | Mean Test Rewards:  -792.0 |\n",
      "| Episode: 1090 | Mean Train Rewards:  -242.8 | Mean Test Rewards:  -820.7 |\n",
      "| Episode: 1100 | Mean Train Rewards:  -246.5 | Mean Test Rewards: -1031.8 |\n",
      "| Episode: 1110 | Mean Train Rewards:  -276.8 | Mean Test Rewards:  -987.8 |\n",
      "| Episode: 1120 | Mean Train Rewards:  -264.1 | Mean Test Rewards:  -891.9 |\n",
      "| Episode: 1130 | Mean Train Rewards:  -249.0 | Mean Test Rewards:  -922.3 |\n",
      "| Episode: 1140 | Mean Train Rewards:  -283.3 | Mean Test Rewards:  -884.1 |\n",
      "| Episode: 1150 | Mean Train Rewards:  -284.0 | Mean Test Rewards:  -721.6 |\n",
      "| Episode: 1160 | Mean Train Rewards:  -238.8 | Mean Test Rewards:  -656.5 |\n",
      "| Episode: 1170 | Mean Train Rewards:  -213.6 | Mean Test Rewards:  -623.5 |\n",
      "| Episode: 1180 | Mean Train Rewards:  -272.2 | Mean Test Rewards:  -645.9 |\n",
      "| Episode: 1190 | Mean Train Rewards:  -293.3 | Mean Test Rewards:  -630.7 |\n",
      "| Episode: 1200 | Mean Train Rewards:  -252.5 | Mean Test Rewards:  -670.3 |\n",
      "| Episode: 1210 | Mean Train Rewards:  -303.2 | Mean Test Rewards:  -688.3 |\n",
      "| Episode: 1220 | Mean Train Rewards:  -323.0 | Mean Test Rewards:  -675.5 |\n",
      "| Episode: 1230 | Mean Train Rewards:  -316.5 | Mean Test Rewards:  -601.3 |\n",
      "| Episode: 1240 | Mean Train Rewards:  -321.2 | Mean Test Rewards:  -613.7 |\n",
      "| Episode: 1250 | Mean Train Rewards:  -305.8 | Mean Test Rewards:  -585.1 |\n",
      "| Episode: 1260 | Mean Train Rewards:  -316.1 | Mean Test Rewards:  -622.1 |\n",
      "| Episode: 1270 | Mean Train Rewards:  -302.8 | Mean Test Rewards:  -642.5 |\n",
      "| Episode: 1280 | Mean Train Rewards:  -299.5 | Mean Test Rewards:  -619.2 |\n",
      "| Episode: 1290 | Mean Train Rewards:  -312.4 | Mean Test Rewards:  -621.4 |\n",
      "| Episode: 1300 | Mean Train Rewards:  -338.3 | Mean Test Rewards:  -636.0 |\n",
      "| Episode: 1310 | Mean Train Rewards:  -357.7 | Mean Test Rewards:  -629.3 |\n",
      "| Episode: 1320 | Mean Train Rewards:  -308.4 | Mean Test Rewards:  -690.3 |\n",
      "| Episode: 1330 | Mean Train Rewards:  -279.0 | Mean Test Rewards:  -651.2 |\n",
      "| Episode: 1340 | Mean Train Rewards:  -258.3 | Mean Test Rewards:  -610.6 |\n",
      "| Episode: 1350 | Mean Train Rewards:  -265.9 | Mean Test Rewards:  -570.0 |\n",
      "| Episode: 1360 | Mean Train Rewards:  -248.4 | Mean Test Rewards:  -606.5 |\n",
      "| Episode: 1370 | Mean Train Rewards:  -267.1 | Mean Test Rewards:  -590.2 |\n",
      "| Episode: 1380 | Mean Train Rewards:  -271.0 | Mean Test Rewards:  -562.3 |\n",
      "| Episode: 1390 | Mean Train Rewards:  -306.4 | Mean Test Rewards:  -529.5 |\n",
      "| Episode: 1400 | Mean Train Rewards:  -326.3 | Mean Test Rewards:  -535.9 |\n",
      "| Episode: 1410 | Mean Train Rewards:  -308.7 | Mean Test Rewards:  -553.5 |\n"
     ]
    }
   ],
   "source": [
    "train_env = gym.make('LunarLander-v3', render_mode=\"rgb_array\")\n",
    "test_env = gym.make('LunarLander-v3', render_mode=\"rgb_array\")\n",
    "\n",
    "SEED = 1234\n",
    "set_seed(SEED)\n",
    "train_env.reset(seed=SEED) # Seed the environment upon reset\n",
    "test_env.reset(seed=SEED+1) # Seed the environment upon reset\n",
    "\n",
    "device = get_device()\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "INPUT_DIM = train_env.observation_space.shape[0]\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = test_env.action_space.n\n",
    "\n",
    "actor = MLP(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "critic = MLP(INPUT_DIM, HIDDEN_DIM, 1)\n",
    "\n",
    "policy = ActorCritic(actor, critic).to(device)\n",
    "policy.apply(init_weights)\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "optimizer = optim.Adam(policy.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "MAX_EPISODES = 1500\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_TRIALS = 25\n",
    "REWARD_THRESHOLD = 200\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "train_rewards = []\n",
    "test_rewards = []\n",
    "\n",
    "for episode in range(1, MAX_EPISODES+1):\n",
    "    policy_loss, value_loss, train_reward = train_episode(train_env, policy, optimizer, DISCOUNT_FACTOR, device)\n",
    "    test_reward = evaluate(test_env, policy, device)\n",
    "    \n",
    "    train_rewards.append(train_reward)\n",
    "    test_rewards.append(test_reward)\n",
    "    \n",
    "    mean_train_rewards = np.mean(train_rewards[-N_TRIALS:])\n",
    "    mean_test_rewards = np.mean(test_rewards[-N_TRIALS:])\n",
    "    \n",
    "    if episode % PRINT_EVERY == 0:    \n",
    "        print(f'| Episode: {episode:3} | Mean Train Rewards: {mean_train_rewards:7.1f} | Mean Test Rewards: {mean_test_rewards:7.1f} |')\n",
    "    \n",
    "    if mean_test_rewards >= REWARD_THRESHOLD:\n",
    "        print(f'Reached reward threshold in {episode} episodes')\n",
    "        break\n",
    "\n",
    "print(f'| Episode: {episode:3} | Mean Train Rewards: {mean_train_rewards:7.1f} | Mean Test Rewards: {mean_test_rewards:7.1f} |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(test_rewards, label='Test Reward')\n",
    "plt.plot(train_rewards, label='Train Reward')\n",
    "plt.xlabel('Episode', fontsize=20)\n",
    "plt.ylabel('Reward', fontsize=20)\n",
    "plt.hlines(REWARD_THRESHOLD, 0, len(test_rewards), color='r')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
